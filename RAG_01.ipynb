{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run a local RAG pipeline from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is RAG?\n",
    "\n",
    "The goal of RAG is to take information and pass it to an LLM so it can generate outputs based on that information.\n",
    "\n",
    "* Retrieval - Find relevant information given a query, e.g. \"what are the macronutrients and what do they do?\" -> retrieves passages of text related to the macronutrients from a nutrition textbook.\n",
    "* Augmented - We want to take the relevant information and augment our input (prompt) to an LLM with that relevant information.\n",
    "* Generation - Take the first two steps and pass them to an LLM for generative outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read where RAG came from, see the paper from Facebook AI: https://arxiv.org/abs/2005.11401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why RAG?\n",
    "\n",
    "The main goal of RAG is to improve the generation outputs of LLMs.\n",
    "\n",
    "1. Prevent hallucinations - LLMs are incredibly good at generating good looking text, however, this text doesn't mean that it's factual. RAG can help LLMs generate information based on relevant passages that are factual.\n",
    "2. Work with custom data - Many base LLMs are trained with internet-scale data. This means they have a fairly good understanding of language in general. However, it also does a lot of their responses can be generic in nature. RAG helps to create specific responses based on specific documents (e.g. your own companies customer support documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can RAG be used for?\n",
    "\n",
    "* Customer support Q&A chat - Treat your existing customer support documents as a resource and when a customer asks a question, you could have a retrieval system, retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\".\n",
    "* Email chain analysis - Let's say you're a large insurance company and you have chains and chains of emails of customer claims. You could use a RAG pipeline to find relevant information from those emails and then use an LLM to process that information into structured data.\n",
    "Company internal documentation chat\n",
    "* Textbook Q&A - Let's say you're a nutrition student and you've got a 1200 page textbook read, you could build a RAG pipeline to go through the textbook and find relevant passages to the questions you have.\n",
    "\n",
    "Common theme here: take your relevant documents to a query and process them with an LLM.\n",
    "\n",
    "From this angle, you can consider an LLM as a calculator for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Local?\n",
    "\n",
    "* Privacy - If you have private documentation, maybe you don't want to send that to an API. You want to setup an LLM and run it on your own hardware.\n",
    "* Speed - Whenever you use an API, you have to send some kind of data across the internet. This takes time. Running locally means we don't have to wait for transfers of data.\n",
    "* Cost - If you own your hardware, the cost is paid. It may have a large cost to begin with. But overtime, you don't have to keep paying API fees.\n",
    "No vendor lockin - If you run your own software/hardware. If OpenAI/another large internet company shut down tomorrow, you can still run your business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
